---
title: "Classification"
author: "Kaiona Apio"
date: "02/24/2025"

format: 
  html: 
    theme: superhero  
    mainfont: monospace
    highlight-style: github
    title-block-banner: true
    embed-resources: true
---

**Abstract:**

This is a technical blog post of **both** an HTML file *and* [.qmd file](https://github.com/kaionaa/DATA505/blob/main/Classification.qmd) hosted on GitHub pages.

# 1. Setup

**Step Up Code:**

```{r, warning=FALSE}
sh <- suppressPackageStartupMessages
sh(library(tidyverse))
sh(library(caret))
sh(library(naivebayes))
sh(library(pROC))
sh(library(forcats))
sh(library(tidytext))
sh(library(SnowballC))
wine <- readRDS(gzcon(url("https://github.com/cd-public/D505/raw/master/dat/pinot.rds")))
```

# 2. Logistic Concepts

Why do we call it Logistic Regression even though we are using the technique for classification?

> Even though we are using this model for classification, we are using a logistic function to model the probability of an event occurring. The logistic function returns a value between 0 and 1 that represents the probability that an event will occur. For example, in this problem, we are using a logistic regression to model the probability that a wine is from Marlborough (the event is a wine being from Marlborough).

# 3. Modeling

We train a logistic regression algorithm to classify a whether a wine comes from Marlborough using:

1.  An 80-20 train-test split.
2.  Three features engineered from the description
3.  5-fold cross validation.

We report Kappa after using the model to predict provinces in the holdout sample.

```{r, warning=FALSE, message=FALSE}
#finding good words
justmar <- wine%>%
  filter(province == "Marlborough")
wine_words <- function(df, j, stem){ 
  words <- df %>%
    unnest_tokens(word, description) %>%
    anti_join(stop_words) %>% 
    filter(!(word %in% c("wine","pinot","vineyard")))
  if(stem){
    words <- words %>% mutate(word = wordStem(word))
  }
  words %>% count(id, word) %>%  group_by(id) %>%  mutate(exists = (n>0)) %>% 
    ungroup %>% group_by(word) %>%  mutate(total = sum(n)) %>% filter(total > j) %>% 
    pivot_wider(id_cols = id, names_from = word, values_from = exists, values_fill = list(exists=0)) %>% 
    right_join(select(df,id,province)) %>% select(-id) %>% mutate(across(-province, ~replace_na(.x, F)))
}

mar_words = as.data.frame(wine_words(df = justmar, j = 45, TRUE))

mar_words = mar_words %>%
  select(!province)

marcounts <- as.data.frame(colSums(mar_words))
marcounts <- tibble::rownames_to_column(marcounts)

ggplot(marcounts, aes(fct_reorder(rowname, `colSums(mar_words)`), `colSums(mar_words)`))+
  geom_col()+
  labs(title="Word Frequencies in Marlborough Pinots", x="popular words", y="counts")+
  theme(
    axis.text = element_text(angle = -45)
  )
  

#Marlborough variable & feature engineering
winmar <- wine%>%
  mutate(
    year = as.factor(year),
    marl = ifelse(province == "Marlborough", "marl", "no:("),
    cherry = str_detect(description,"cherr"), 
    mid = str_detect(description,"medium"),
    silky = str_detect(description,"silky"))%>%
  select(-province, -description, -id)
head(winmar)

#train & test split
set.seed(505)
winmar_index <- createDataPartition(winmar$marl, p=0.80, list=FALSE)
trainmar <- winmar[winmar_index, ]
testmar <- winmar[-winmar_index, ]
table(trainmar$marl)


#5-fold cv training
control = trainControl(method = "cv", number = 5)
get_fit <- function(trainmar) {
  train(marl ~ .,
        data = trainmar, 
        trControl = control,
        method = "glm",
        family = "binomial",
        maxit = 5)
}
fitmar <- get_fit(trainmar)
print(fitmar)
```

# 4. Binary vs Other Classification

What is the difference between determining some form of classification through logistic regression versus methods like $K$-NN and Naive Bayes which performed classifications.

> As mentioned in #2, logistic regression models the probability of an event occurring given the underlying distribution of the data. On the other hand, both KNN and Naive Bayes classify a new instance, for KNN it is based on the nearest neighbors, and for Naive Bayes it is based on the probability of each class based on the features of a given instance. The main difference is that logistic regression is a statistical method that uses assumptions about the features and target class to determine probability.

# 5. ROC Curves

We can display an ROC for the model to explain your model's quality.

```{r, warning=FALSE, message=FALSE}
# You can find a tutorial on ROC curves here: https://towardsdatascience.com/understanding-the-roc-curve-and-auc-dd4f9a192ecb/

prob <- predict(fitmar, newdata = testmar, type = "prob")[,2]
myRoc <- roc(testmar$marl, prob)
AUC <-auc(myRoc)

ggroc(myRoc, colour = 'darkgreen', size = 2) +
  ggtitle(paste0('ROC Curve ', '(AUC = ', AUC, ')'))+
  theme_light()+
  theme(plot.title = element_text(face = "bold"))
```

> The ROC curve measures the sensitivity (how well the model detects true positives a.k.a. 'right answers') and specificity (how well the model avoids false positives a.k.a 'wrong answers') of a model. Since I focused on words commonly found in the descriptions of Marlborough wines, I have a larger area under the cure (an A- if you will). This means that the model is moderately good at predicting if a wine is from Marlborough.
